# RAG Based Chatbot:
#### _where you can chat with PDF installation manuals_

When thinking about creating a chatbot, the very first problem that we should be concerned about is understanding the input.
Chatbots are usually text-to-text, where basically, a user inputs some text and then gets new text as an answer.
But computers don't understand language, the only thing they can understand are `numbers`.

# Tokenizer
In LLM's, people often use `tokens`, where each token is a number, something a computer understands, hence, a `String` of text gets converted into an array of `tokens`.
We can use `tokens` when:
- Training the model with data, which will be turned into a list of tokens 
- The model infers (generates) answers, which will then get decoded into text

[Tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt-4o) is an amazing website to experiment with different tokenizers.

This chatbot will be primarily focused on home equipment installation guides, it's not going to be as diverse as the available online chatbots (ChatGPT, Gemini, Claude, etc...), as it will only focus on home equipment installation, ignoring everything else that is not relevant.

It will be wise to train the model with data relevant to home equipment and not include data about let's say animals or cars.

As tempting as it is to just use the latest tokenizers, like gpt-4o, it would be smarter to use a smaller tokenizer (basically with less `total token size` or sometimes called `vocal_size`).
The reason for this is because, the bigger our `total token size` the more computational power is needed to train the model and to infer answers.
I would use a modified version of the `GPT-2` tokenizer, that's more focused on home equipment, as GPT-2 has a smaller total token size (which is around 50,000, while GPT-4o has something around 128,000 `I think`).

I suggest modified, because GPT-2's tokenizer generates smaller tokens instead of GPT-4o's generated tokens, which worsens the `self-attention` property of `transformers`.
For example, if we have a `100` word text, gpt-4o might convert it into a list of `30` tokens, while gpt-2 might convert it into something around `50` tokens.

If possible we could delete some tokens that are not related to house equipment and replace them with more related words or parts of words (or we could make our own unique tokenizer).

Yes, this will cause some minor problems, they will be covered later.

# Embedding

Once we have decided on our tokenizer, we will need to embed each token in an n-dimensional space.

We would love to use `transformer embeddings` or `bag-of-words embeddings`, used by OpenAI for ChatGPT.
These embeddings are amazing, as they use the `transformer neural network`, which introduces `self-attention`, this gives the ability for each token to see other tokens and change its value, or meaning, affected by nearby (or far) tokens.

For instance:
 - The brown fox jumped, `it` almost fell
 - A butterfly was nearby, `it` was very colorful
 - The house was very expensive but `it` had an amazing view

In these three sentences, `it` was used, but `it` refers to a different object, animal, or `token` in each context.
Transformers give the ability to differentiate `it` from each context, with 2 features:
- The position of the token is taken into consideration when input is given to the neural network
- `self-attention` gives the ability for `it` to see the words preceding it, which are tokens or combinations of tokens `brown fox`, `butterfly`, `house`

Because we want our users to chat with PDFs, the chatbot needs to "understand" English (understand the sequence of tokens generated by our input text).
When we ask a question like "What should I do before installing the Air conditioner?", the chatbot should understand the sequence of words (tokens) that we wrote to give a relative answer.

# Vector database
It would be convenient to store the vector (embedding) of `each token` in a file, to not have to train the model from `0` every time we want to use the model, also to be able to adjust these values when training with new data.

The dimension of each vector (which will all be the same dimension), depends on how much depth we want in our model.

GPT-2 uses something around `12,000` dimensions per vector, we most likely won't need that many dimensions, as our focus is narrowed down to installing house equipment.
Finding the sweet spot usually requires trial and error, as there isn't a general number we can use as our dimensions (or any other hyperparameter).

each parameter (in this case, if we use 12,000 dimension vectors, each token will have 12,000 parameters), will take 2, 4, or even 8 `bytes` of storage, depending on our implementation, we may need more than 100 GBs of storage to store all the parameters.

# Training the model
- `pre-training`

We have the tokenizer and the embeddings, we need to train the model with data `relevant to installing house equipment` and some small amounts of data about general English or communication, so the model gets a better idea of how words work together.

It is possible to use any data available on the internet about house equipment (some data specifically not related to installing an equipment but details about it would also be nice for more diversity about house equipment), however, we should be careful about the data that we feed into the machine, as there could be wrong or "bad" data.

- `fine-tuning`

Data is not enough, as the model will not be able to give an answer like we humans expect (it might give more questions after we ask a question), this is where fine-tuning is important.
We can come up with conversations or Q&As specifically similar to giving instructions and feed that to the model to teach it how to give an answer like a human.

It is not necessary to feed large amounts of prompts to the model, but the quality of the prompts is very important

# Generating a response
Response generation in numbered bullet points format would be very convenient in our specific case because we are training the model to act as an installation manual PDF file, usually, manuals have a step-by-step guidance for installing an equipment.

OpenAI's ChatGPT uses this format as well! 
We would like to mimic this response behavior.

The `type of response` from the model usually comes from the `fine-tuning` stage during training, that is when we define the "personality" of the model or the type of answers it generates with its knowledge.



# PDF processing
Our model can take text => convert it into tokens (encode it) => understand the context (thanks to transformers) => generate new tokens relevant to the input => decode it => give it to the user.

A straightforward approach to PDF processing is taking the PDF file, reading all the text in that PDF file (we will have a huge String variable), encoding it and taking that as input with the original input from the user, to then generate an answer.

The lack of comprehension of images may give the model a harder time processing a PDF file in a correct way, I will discuss this in more detail later.

# Challenges
Of course, everything comes with a cost, as much as we try to make the best model we can, we will face some challenges.

- The model is at risk of "overfitting" its vocabulary to certain keywords

Because we narrowed down our tokens, if someone asks a question or uses a word unrelated to house equipment, the chatbot will not be able to give an answer, of course, this is not a huge concern, but it starts to get problematic when a user has to use certain keywords to get something meaningful out of the chatbot.

This problem could arise if we are not careful with our training data and the "words" we feed the model

- The model will require a specific PDF format to understand its context well

Because our model doesn't understand images, when a user sends a PDF installation manual with a bunch of pictures and less text, the model will not be able to comprehend the PDF file and will most likely give a wrong answer to the user's question.

- large PDF files may interrupt the model's attention

If a user sends an enormous PDF file with a million words, the model will reach its `context_size` limit and forget the original question asked by the user.
This will cause the model to generate an answer completely unrelated to the user's prompt.

- The model may get confused with some data

Manuals often have pictures with floating text next to it, the model will not understand that the given text is related to a certain image, it will just see the text as randomly written text, for this reason, the model may pick up weird relations between words that are not true.

# Prompt examples
Our model will do amazing when given PDF files that are `text focused`, otherwise it will struggle.
The model may also struggle with prompts that are almost off-topic to house equipment.

### 5 examples that the chatbot can answer:
- "How can I install this air conditioner using the manual I gave you?": This depends on the given PDF file, but oftentimes, manuals have a step-by-step explanation of how to do things and our model is designed to read and give instructions in a step-by-step manner.
- "Is there anything I should be cautious of when flaring the pipe end?": Usually, right after a step in the process of installing an equipment, there is a list of warnings to be aware of written in `text`, not in images, the model could easily pick that up and give that list as a response.
- "Can you summarize this manual?": Manuals often have numbered topics, the model could easily recognize all the topics present in the manual and give an answer instantly, this is often reliant on how the fine-tuning was done.
- "What accessories do I need?": A manual often has a list of accessories needed with their quantity, the model can easily understand when the list stops, as the quantity number `token` (if they are defined to be tokens) will stop appearing in the sequence as the list of accessories ends.
- "How can I make sure that I installed the Air conditioner correctly?": Usually at the end of a manual, there are ways of testing the equipment to make sure it's installed properly.

### 5 examples where the chatbot struggles:
- "Which end of the pipe should I cut?": Sometimes, text isn't enough to explain the exact way of doing things, sometimes it's easier to explain a step with a picture (which many manuals do), the model won't be able to understand what the picture is presenting, even if there is text next to the image, as the model can't tell if this specific text relates to that image. There is also the risk of the model giving a completely wrong answer because maybe 2 pages above it's mentioned to cut in a different way for a different pipe.
- "What are the insulation dimensions that I should provide?": (this is at the bottom of page 6 from the given installation manual), the information the user is asking for is given in a table format, it may be very difficult for the model to comprehend a table, as it's not structured in a linear, left-to-right way like regular text.
- "How many pages is the manual?": The model is not trained to understand the number of pages in a PDF file, although we could train it to do so when fine-tuning, every PDF file has different tokens at the end (it could be the last page number, or it could be other text or numbers), it's hard to generalize a way to understand PDF page size.
- "Does the manual say the exact same thing in all the provided languages?": Depending on the way we train our model, it should be able to understand different languages, however, it isn't trained to translate and compare different languages, that's called `cross-attention` and is not needed for our purposes.
- "How do I make a lemonade?": The chatbot is trained with data relevant to home equipment installation, it simply doesn't have the knowledge on what a lemonade is.
# References

- [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- [Attention in transformers, visually explained | Chapter 6, Deep Learning](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g&t=1208s)

